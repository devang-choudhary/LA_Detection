{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(a,project):\n",
    "    # load the model\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    # drop all rows in df\n",
    "    df = df.drop(df.index)\n",
    "    print(df.shape)\n",
    "    for p,u in a.items():\n",
    "        if p == project:\n",
    "            continue\n",
    "        df1 = pd.read_csv(f'../data/{p}_final.csv')\n",
    "        df = pd.concat([df,df1])\n",
    "    # \"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\",content,path\n",
    "    df = df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    # Count the number of rows in the \"not_smell\" class\n",
    "    # not_smell_count = (df['catagory'] == 'not_smell').sum()\n",
    "\n",
    "    # # Calculate the number of rows to drop (one-third of the count)\n",
    "    # rows_to_drop = not_smell_count // 3\n",
    "\n",
    "    # # Filter the DataFrame to exclude one-third of the \"not_smell\" class\n",
    "    # filtered_smell = df[df['catagory'] == 'not_smeel'].sample(frac=(1/3))  # Selecting 2/3 of 'not_smell' instances\n",
    "    # # print(filtered_smell.shape)\n",
    "    \n",
    "    # defalut_smell = df[df['catagory'] != 'not_smeel']\n",
    "    # print(defalut_smell.shape)\n",
    "    # df = pd.concat([filtered_smell, defalut_smell])\n",
    "    \n",
    "    train_df = shuffle(df)\n",
    "    test_df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    test_df = test_df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    # print(df['catagory'].value_counts())\n",
    "    \n",
    "    # df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # np.random.seed(42)\n",
    "    # p = 0.1 # 10% for test set\n",
    "    # prop = 1-p\n",
    "    # temp_df = df.copy()\n",
    "    # msk = np.random.rand(len(temp_df)) < prop\n",
    "    # train_df = temp_df[msk]\n",
    "    # test_df = temp_df[~msk]\n",
    "    # print(train_df[\"label\"].value_counts())\n",
    "    # print(test_df[\"label\"].value_counts())\n",
    "    # test_df['index'] = test_df.index\n",
    "    # train_df['index'] = train_df.index  \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_temp(a,project):\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1,inplace=True)\n",
    "    train_df, test_df = train_test_split(df, train_size=.8, random_state=42)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "def model_(train_df,test_df):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "    X_train = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "    X_test = tfidf_vectorizer.transform(test_df['content'])\n",
    "    train_df.drop(columns=['content'], inplace=True)\n",
    "    test_df.drop(columns=['content'], inplace=True)\n",
    "    y_train = train_df.values\n",
    "    y_test = test_df.values\n",
    "    # Create the MultiOutputClassifier\n",
    "    log_reg = LogisticRegression(max_iter=1000) \n",
    "    multi_target_log_reg = MultiOutputClassifier(log_reg, n_jobs=-1)\n",
    "\n",
    "    # Train the model\n",
    "    multi_target_log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = multi_target_log_reg.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, predictions, target_names=['get','DOS','is','set']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"buildship\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"eclips-collections\": \"https://github.com/eclipse/eclipse-collections.git\",\n",
    "    \"hawkbit\": \"https://github.com/eclipse/hawkbit.git\",\n",
    "    \"jkube\": \"https://github.com/eclipse/jkube.git\",\n",
    "    \"kura\": \"https://github.com/eclipse/kura.git\",\n",
    "    \"jifa\": \"https://github.com/eclipse/microprofile.git\",\n",
    "    \"milo\": \"https://github.com/eclipse/milo.git\",\n",
    "    \"openvsx\": \"https://github.com/eclipse/openvsx.git\",\n",
    "    \"steady\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"xtext\": \"https://github.com/eclipse/xtext.git\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3904, 5)\n",
      "(977, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.00      0.00      0.00        31\n",
      "         DOS       0.98      0.90      0.94       146\n",
      "          is       0.50      0.40      0.44       257\n",
      "         set       0.60      0.24      0.34       114\n",
      "\n",
      "   micro avg       0.67      0.48      0.56       548\n",
      "   macro avg       0.52      0.38      0.43       548\n",
      "weighted avg       0.62      0.48      0.53       548\n",
      " samples avg       0.26      0.26      0.26       548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5976, 5)\n",
      "(1495, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.78      0.22      0.35       175\n",
      "         DOS       0.00      0.00      0.00        33\n",
      "          is       0.83      0.61      0.70       206\n",
      "         set       0.82      0.49      0.61       468\n",
      "\n",
      "   micro avg       0.82      0.45      0.58       882\n",
      "   macro avg       0.61      0.33      0.42       882\n",
      "weighted avg       0.78      0.45      0.56       882\n",
      " samples avg       0.26      0.26      0.26       882\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2543, 5)\n",
      "(636, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.80      0.20      0.32        59\n",
      "         DOS       0.92      0.45      0.61        51\n",
      "          is       0.48      0.29      0.36       141\n",
      "         set       0.66      0.50      0.57       157\n",
      "\n",
      "   micro avg       0.64      0.38      0.48       408\n",
      "   macro avg       0.72      0.36      0.47       408\n",
      "weighted avg       0.65      0.38      0.47       408\n",
      " samples avg       0.24      0.23      0.23       408\n",
      "\n",
      "(2481, 5)\n",
      "(621, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.00      0.00      0.00        37\n",
      "         DOS       1.00      0.44      0.61        16\n",
      "          is       0.43      0.19      0.27       196\n",
      "         set       0.64      0.47      0.54       177\n",
      "\n",
      "   micro avg       0.57      0.30      0.39       426\n",
      "   macro avg       0.52      0.28      0.35       426\n",
      "weighted avg       0.50      0.30      0.37       426\n",
      " samples avg       0.21      0.18      0.19       426\n",
      "\n",
      "(16195, 5)\n",
      "(4049, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.85      0.68      0.76       563\n",
      "         DOS       0.81      0.77      0.79       459\n",
      "          is       0.51      0.16      0.25       833\n",
      "         set       0.50      0.28      0.36       419\n",
      "\n",
      "   micro avg       0.72      0.44      0.54      2274\n",
      "   macro avg       0.67      0.47      0.54      2274\n",
      "weighted avg       0.66      0.44      0.50      2274\n",
      " samples avg       0.24      0.24      0.24      2274\n",
      "\n",
      "(616, 5)\n",
      "(154, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       1.00      0.14      0.25         7\n",
      "         DOS       1.00      0.83      0.91        12\n",
      "          is       0.00      0.00      0.00        44\n",
      "         set       1.00      0.07      0.13        43\n",
      "\n",
      "   micro avg       0.93      0.13      0.23       106\n",
      "   macro avg       0.75      0.26      0.32       106\n",
      "weighted avg       0.58      0.13      0.17       106\n",
      " samples avg       0.09      0.09      0.09       106\n",
      "\n",
      "(8179, 5)\n",
      "(2045, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.79      0.47      0.59        49\n",
      "         DOS       0.88      0.92      0.90       201\n",
      "          is       0.47      0.38      0.42       650\n",
      "         set       0.60      0.39      0.47       201\n",
      "\n",
      "   micro avg       0.60      0.49      0.54      1101\n",
      "   macro avg       0.69      0.54      0.60      1101\n",
      "weighted avg       0.58      0.49      0.53      1101\n",
      " samples avg       0.26      0.26      0.26      1101\n",
      "\n",
      "(2436, 5)\n",
      "(610, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.96      0.69      0.80        77\n",
      "         DOS       1.00      0.98      0.99        54\n",
      "          is       0.31      0.11      0.17        97\n",
      "         set       0.43      0.12      0.19        84\n",
      "\n",
      "   micro avg       0.76      0.41      0.53       312\n",
      "   macro avg       0.68      0.48      0.54       312\n",
      "weighted avg       0.62      0.41      0.47       312\n",
      " samples avg       0.21      0.21      0.21       312\n",
      "\n",
      "(4470, 5)\n",
      "(1118, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.56      0.21      0.31       104\n",
      "         DOS       0.94      0.82      0.87       144\n",
      "          is       0.50      0.37      0.43       265\n",
      "         set       0.62      0.18      0.28       113\n",
      "\n",
      "   micro avg       0.65      0.41      0.51       626\n",
      "   macro avg       0.66      0.40      0.47       626\n",
      "weighted avg       0.63      0.41      0.48       626\n",
      " samples avg       0.23      0.23      0.23       626\n",
      "\n",
      "(72731, 5)\n",
      "(18183, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.82      0.72      0.77      3297\n",
      "         DOS       0.76      0.37      0.50       793\n",
      "          is       0.56      0.04      0.08      3545\n",
      "         set       0.59      0.15      0.24      2421\n",
      "\n",
      "   micro avg       0.77      0.32      0.45     10056\n",
      "   macro avg       0.68      0.32      0.40     10056\n",
      "weighted avg       0.67      0.32      0.38     10056\n",
      " samples avg       0.17      0.17      0.17     10056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for project, url in a.items():\n",
    "    train_df, test_df = get_df_temp(a,project)\n",
    "    model_(train_df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
