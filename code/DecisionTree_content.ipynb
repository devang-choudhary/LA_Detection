{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(a,project):\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    df = df.drop(df.index)\n",
    "    print(df.shape)\n",
    "    for p,u in a.items():\n",
    "        if p == project:\n",
    "            continue\n",
    "        df1 = pd.read_csv(f'../data/{p}_final.csv')\n",
    "        df = pd.concat([df,df1])\n",
    "    df = df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    train_df = shuffle(df)\n",
    "    test_df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    test_df = test_df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_temp(a,project):\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1,inplace=True)\n",
    "    train_df, test_df = train_test_split(df, train_size=.8, random_state=42)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "def model_(train_df,test_df):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "    X_train = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "    X_test = tfidf_vectorizer.transform(test_df['content'])\n",
    "    train_df.drop(columns=['content'], inplace=True)\n",
    "    test_df.drop(columns=['content'], inplace=True)\n",
    "    y_train = train_df.values\n",
    "    y_test = test_df.values\n",
    "    # Create the MultiOutputClassifier\n",
    "    clf = DecisionTreeClassifier()\n",
    "    multi_target_log_reg = MultiOutputClassifier(clf, n_jobs=-1)\n",
    "\n",
    "    # Train the model\n",
    "    multi_target_log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = multi_target_log_reg.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, predictions, target_names=['get','DOS','is','set']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"buildship\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"eclips-collections\": \"https://github.com/eclipse/eclipse-collections.git\",\n",
    "    \"hawkbit\": \"https://github.com/eclipse/hawkbit.git\",\n",
    "    \"jkube\": \"https://github.com/eclipse/jkube.git\",\n",
    "    \"kura\": \"https://github.com/eclipse/kura.git\",\n",
    "    \"jifa\": \"https://github.com/eclipse/microprofile.git\",\n",
    "    \"milo\": \"https://github.com/eclipse/milo.git\",\n",
    "    \"openvsx\": \"https://github.com/eclipse/openvsx.git\",\n",
    "    \"steady\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"xtext\": \"https://github.com/eclipse/xtext.git\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3904, 5)\n",
      "(977, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.00      0.00      0.00        31\n",
      "         DOS       0.96      0.95      0.96       146\n",
      "          is       0.26      0.19      0.22       257\n",
      "         set       0.73      0.74      0.73       114\n",
      "\n",
      "   micro avg       0.59      0.49      0.54       548\n",
      "   macro avg       0.49      0.47      0.48       548\n",
      "weighted avg       0.53      0.49      0.51       548\n",
      " samples avg       0.26      0.26      0.26       548\n",
      "\n",
      "(5976, 5)\n",
      "(1495, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.82      0.78      0.80       175\n",
      "         DOS       0.54      0.39      0.46        33\n",
      "          is       0.89      0.81      0.85       206\n",
      "         set       0.70      0.74      0.72       468\n",
      "\n",
      "   micro avg       0.76      0.75      0.75       882\n",
      "   macro avg       0.74      0.68      0.71       882\n",
      "weighted avg       0.76      0.75      0.75       882\n",
      " samples avg       0.44      0.44      0.44       882\n",
      "\n",
      "(2543, 5)\n",
      "(636, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.71      0.54      0.62        59\n",
      "         DOS       0.73      0.65      0.69        51\n",
      "          is       0.26      0.23      0.24       141\n",
      "         set       0.76      0.75      0.75       157\n",
      "\n",
      "   micro avg       0.58      0.52      0.55       408\n",
      "   macro avg       0.62      0.54      0.57       408\n",
      "weighted avg       0.58      0.52      0.55       408\n",
      " samples avg       0.31      0.31      0.30       408\n",
      "\n",
      "(2481, 5)\n",
      "(621, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.54      0.51      0.53        37\n",
      "         DOS       0.62      0.62      0.62        16\n",
      "          is       0.22      0.20      0.21       196\n",
      "         set       0.74      0.70      0.72       177\n",
      "\n",
      "   micro avg       0.48      0.45      0.47       426\n",
      "   macro avg       0.53      0.51      0.52       426\n",
      "weighted avg       0.48      0.45      0.47       426\n",
      " samples avg       0.28      0.27      0.26       426\n",
      "\n",
      "(16195, 5)\n",
      "(4049, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.87      0.86      0.87       563\n",
      "         DOS       0.81      0.90      0.85       459\n",
      "          is       0.22      0.11      0.15       833\n",
      "         set       0.72      0.70      0.71       419\n",
      "\n",
      "   micro avg       0.67      0.56      0.61      2274\n",
      "   macro avg       0.65      0.64      0.64      2274\n",
      "weighted avg       0.59      0.56      0.57      2274\n",
      " samples avg       0.30      0.30      0.29      2274\n",
      "\n",
      "(616, 5)\n",
      "(154, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       1.00      0.71      0.83         7\n",
      "         DOS       0.79      0.92      0.85        12\n",
      "          is       0.10      0.07      0.08        44\n",
      "         set       0.74      0.58      0.65        43\n",
      "\n",
      "   micro avg       0.54      0.42      0.47       106\n",
      "   macro avg       0.66      0.57      0.60       106\n",
      "weighted avg       0.50      0.42      0.45       106\n",
      " samples avg       0.28      0.25      0.26       106\n",
      "\n",
      "(8179, 5)\n",
      "(2045, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.78      0.78      0.78        49\n",
      "         DOS       0.95      0.95      0.95       201\n",
      "          is       0.37      0.20      0.26       650\n",
      "         set       0.74      0.65      0.69       201\n",
      "\n",
      "   micro avg       0.63      0.45      0.52      1101\n",
      "   macro avg       0.71      0.65      0.67      1101\n",
      "weighted avg       0.56      0.45      0.49      1101\n",
      " samples avg       0.23      0.23      0.23      1101\n",
      "\n",
      "(2436, 5)\n",
      "(610, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.94      0.82      0.88        77\n",
      "         DOS       0.90      0.98      0.94        54\n",
      "          is       0.32      0.37      0.34        97\n",
      "         set       0.64      0.61      0.62        84\n",
      "\n",
      "   micro avg       0.64      0.65      0.64       312\n",
      "   macro avg       0.70      0.69      0.69       312\n",
      "weighted avg       0.66      0.65      0.65       312\n",
      " samples avg       0.33      0.31      0.32       312\n",
      "\n",
      "(4470, 5)\n",
      "(1118, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.81      0.65      0.72       104\n",
      "         DOS       0.89      0.92      0.90       144\n",
      "          is       0.28      0.26      0.27       265\n",
      "         set       0.65      0.66      0.66       113\n",
      "\n",
      "   micro avg       0.58      0.55      0.56       626\n",
      "   macro avg       0.66      0.63      0.64       626\n",
      "weighted avg       0.57      0.55      0.56       626\n",
      " samples avg       0.29      0.30      0.29       626\n",
      "\n",
      "(72731, 5)\n",
      "(18183, 5)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.93      0.89      0.91      3297\n",
      "         DOS       0.87      0.71      0.78       793\n",
      "          is       0.26      0.06      0.10      3545\n",
      "         set       0.68      0.42      0.52      2421\n",
      "\n",
      "   micro avg       0.77      0.47      0.59     10056\n",
      "   macro avg       0.68      0.52      0.58     10056\n",
      "weighted avg       0.63      0.47      0.52     10056\n",
      " samples avg       0.26      0.25      0.25     10056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for project, url in a.items():\n",
    "    train_df, test_df = get_df_temp(a,project)\n",
    "    model_(train_df,test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
