{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(a,project):\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    df = df.drop(df.index)\n",
    "    print(df.shape)\n",
    "    for p,u in a.items():\n",
    "        if p == project:\n",
    "            continue\n",
    "        df1 = pd.read_csv(f'../data/{p}_final.csv')\n",
    "        df = pd.concat([df,df1])\n",
    "    df = df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    train_df = shuffle(df)\n",
    "    test_df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    test_df = test_df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_temp(a,project):\n",
    "    df = pd.read_csv(f'../data/{project}_final.csv')\n",
    "    df.drop(columns=[\"access_modifier\",\"return_type\",\"method_name\",\"parameters\",\"returns\",\"comments\"],axis=1,inplace=True)\n",
    "    train_df, test_df = train_test_split(df, train_size=.8, random_state=42)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "def model_(train_df,test_df):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "    X_train = tfidf_vectorizer.fit_transform(train_df['content'])\n",
    "    X_test = tfidf_vectorizer.transform(test_df['content'])\n",
    "    train_df.drop(columns=['content'], inplace=True)\n",
    "    test_df.drop(columns=['content'], inplace=True)\n",
    "    y_train = train_df.values\n",
    "    y_test = test_df.values\n",
    "    # Create the MultiOutputClassifier\n",
    "    adaboost_clf = AdaBoostClassifier(n_estimators=30)\n",
    "    multi_target_svm = MultiOutputClassifier(adaboost_clf, n_jobs=-1)\n",
    "\n",
    "    # Train the model\n",
    "    multi_target_svm.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = multi_target_svm.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, predictions, target_names=['get','DOS','is','set']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\n",
    "    \"buildship\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"eclips-collections\": \"https://github.com/eclipse/eclipse-collections.git\",\n",
    "    \"hawkbit\": \"https://github.com/eclipse/hawkbit.git\",\n",
    "    \"jkube\": \"https://github.com/eclipse/jkube.git\",\n",
    "    \"kura\": \"https://github.com/eclipse/kura.git\",\n",
    "    \"jifa\": \"https://github.com/eclipse/microprofile.git\",\n",
    "    \"milo\": \"https://github.com/eclipse/milo.git\",\n",
    "    \"openvsx\": \"https://github.com/eclipse/openvsx.git\",\n",
    "    \"steady\": \"https://github.com/eclipse/steady.git\",\n",
    "    \"xtext\": \"https://github.com/eclipse/xtext.git\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.00      0.00      0.00       115\n",
      "         DOS       0.98      0.73      0.84       767\n",
      "          is       0.40      0.01      0.02      1282\n",
      "         set       0.40      0.08      0.14       559\n",
      "\n",
      "   micro avg       0.70      0.23      0.34      2723\n",
      "   macro avg       0.45      0.21      0.25      2723\n",
      "weighted avg       0.55      0.23      0.27      2723\n",
      " samples avg       0.13      0.12      0.13      2723\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.64      0.01      0.02       857\n",
      "         DOS       0.02      0.48      0.04       143\n",
      "          is       0.00      0.00      0.00      1070\n",
      "         set       0.33      0.00      0.00      2362\n",
      "\n",
      "   micro avg       0.03      0.02      0.02      4432\n",
      "   macro avg       0.25      0.12      0.02      4432\n",
      "weighted avg       0.30      0.02      0.01      4432\n",
      " samples avg       0.01      0.01      0.01      4432\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.49      0.57      0.53       272\n",
      "         DOS       0.99      0.55      0.70       249\n",
      "          is       0.00      0.00      0.00       753\n",
      "         set       0.57      0.27      0.36       756\n",
      "\n",
      "   micro avg       0.60      0.24      0.35      2030\n",
      "   macro avg       0.51      0.35      0.40      2030\n",
      "weighted avg       0.40      0.24      0.29      2030\n",
      " samples avg       0.16      0.15      0.15      2030\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.42      0.34      0.38       154\n",
      "         DOS       1.00      0.33      0.50       106\n",
      "          is       0.25      0.00      0.00      1032\n",
      "         set       0.42      0.06      0.10       850\n",
      "\n",
      "   micro avg       0.49      0.06      0.11      2142\n",
      "   macro avg       0.52      0.18      0.24      2142\n",
      "weighted avg       0.36      0.06      0.09      2142\n",
      " samples avg       0.04      0.04      0.04      2142\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.63      0.81      0.71      2679\n",
      "         DOS       0.93      0.41      0.57      2285\n",
      "          is       0.25      0.01      0.01      4182\n",
      "         set       0.39      0.13      0.20      2289\n",
      "\n",
      "   micro avg       0.64      0.30      0.41     11435\n",
      "   macro avg       0.55      0.34      0.37     11435\n",
      "weighted avg       0.50      0.30      0.32     11435\n",
      " samples avg       0.17      0.17      0.17     11435\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.75      0.43      0.55        28\n",
      "         DOS       1.00      0.63      0.77        43\n",
      "          is       0.14      0.00      0.01       209\n",
      "         set       0.47      0.05      0.09       178\n",
      "\n",
      "   micro avg       0.71      0.11      0.19       458\n",
      "   macro avg       0.59      0.28      0.35       458\n",
      "weighted avg       0.39      0.11      0.15       458\n",
      " samples avg       0.06      0.06      0.06       458\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.25      0.83      0.39       312\n",
      "         DOS       0.96      0.04      0.08      1064\n",
      "          is       0.14      0.00      0.00      3292\n",
      "         set       0.43      0.10      0.16       911\n",
      "\n",
      "   micro avg       0.31      0.07      0.11      5579\n",
      "   macro avg       0.45      0.24      0.16      5579\n",
      "weighted avg       0.35      0.07      0.06      5579\n",
      " samples avg       0.04      0.04      0.04      5579\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.73      0.87      0.79       345\n",
      "         DOS       0.92      0.33      0.49       293\n",
      "          is       0.00      0.00      0.00       513\n",
      "         set       0.80      0.02      0.04       404\n",
      "\n",
      "   micro avg       0.76      0.26      0.39      1555\n",
      "   macro avg       0.61      0.31      0.33      1555\n",
      "weighted avg       0.54      0.26      0.28      1555\n",
      " samples avg       0.13      0.13      0.13      1555\n",
      "\n",
      "(0, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.65      0.65      0.65       494\n",
      "         DOS       0.97      0.73      0.84       767\n",
      "          is       0.40      0.01      0.02      1282\n",
      "         set       0.34      0.08      0.13       559\n",
      "\n",
      "   micro avg       0.76      0.30      0.43      3102\n",
      "   macro avg       0.59      0.37      0.41      3102\n",
      "weighted avg       0.57      0.30      0.34      3102\n",
      " samples avg       0.17      0.17      0.17      3102\n",
      "\n",
      "(0, 11)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         get       0.94      0.59      0.73     16416\n",
      "         DOS       0.69      0.51      0.59      4084\n",
      "          is       0.47      0.38      0.42     17948\n",
      "         set       0.35      0.06      0.10     12075\n",
      "\n",
      "   micro avg       0.65      0.38      0.48     50523\n",
      "   macro avg       0.61      0.39      0.46     50523\n",
      "weighted avg       0.61      0.38      0.46     50523\n",
      " samples avg       0.21      0.21      0.21     50523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/devang/miniconda3/envs/flask/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for project, url in a.items():\n",
    "    train_df, test_df = get_df(a,project)\n",
    "    model_(train_df,test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
